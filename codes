

#####################################################
# Import Libraries
#####################################################

import itertools
import warnings
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import statsmodels.api as sm
from sklearn.metrics import mean_absolute_error
# !pip install lightgbm
# conda install lightgbm
import time
import seaborn as sns
import lightgbm as lgb
import warnings

pd.set_option('display.max_columns', None)
pd.set_option('display.width', 500)
warnings.filterwarnings('ignore')

########################
# Loading the data
########################

DataFrame = pd.read_csv('iyzico_data.csv', index_col=0)
df = DataFrame.copy()

df["transaction_date"] = df["transaction_date"].apply(pd.to_datetime)

########################
# EDA (Exploratory Data Analysis)
########################

def check_df(dataframe, head=5):
    print("##################### Shape #####################")
    print(dataframe.shape)
    print("##################### Types #####################")
    print(dataframe.dtypes)
    print("##################### Head #####################")
    print(dataframe.head(head))
    print("##################### Tail #####################")
    print(dataframe.tail(head))
    print("##################### NA #####################")
    print(dataframe.isnull().sum())
    print("##################### Quantiles #####################")
    print(dataframe.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)

check_df(df)

df['transaction_date'].min(), df['transaction_date'].max()
#Dates are 3 years between 01.01.2018 and 31.12.2020.

df['merchant_id'].nunique()
#7 unique merchant

df.groupby('merchant_id').agg({"Total_Transaction": ["sum", "mean", "median", "std"]})

df["Total_Transaction"].sum()
#8.391.252 unique transaction
df["Total_Paid"].sum()
#3.558.840.563 total_paid

df.groupby('merchant_id').agg({"Total_Paid": ["sum", "mean", "median", "std"]})

########################
# DATA VISULATION
########################
df["Total_Transaction"].plot(figsize=(15, 6))
plt.xlabel("Date")
plt.ylabel("Total Transaction")
plt.title("İyzico Total Transaction")
x = df["transaction_date"]
plt.tight_layout()
plt.show()


df["Total_Paid"].plot(figsize=(15, 6))
plt.xlabel("Date")
plt.ylabel("Total Paid")
plt.title("İyzico Total Paid")
x = df["transaction_date"]
plt.tight_layout()
plt.show()

df.groupby(["merchant_id"]).agg({
    "Total_Transaction": ["sum", "mean"],
    "Total_Paid": ["sum", "mean"]})

sns.boxplot(data = df, x = df["merchant_id"], y = df["Total_Transaction"])
plt.show(block=True)

########################
# FEATURE  ENGINEERING
########################

usd = pd.read_csv('US_Dollar_Index.csv')
usd["Date"] = usd["Date"].apply(pd.to_datetime)
usd.head()
usd.columns = usd.columns.str.replace('Date', 'transaction_date')
df = df.merge(usd, how='outer', on='transaction_date')
df.head()
df.drop(["Open","High","Low","Vol.","Change %"], axis = 1, inplace = True)


df.groupby(["transaction_date"]).agg({
    "Total_Transaction": ["sum", "mean"],
    "Total_Paid": ["sum", "mean"]})



df.head()
#Creating features about time
def create_date_features(df):
    df['month'] = df.transaction_date.dt.month
    df['day_of_month'] = df.transaction_date.dt.day
    df['day_of_year'] = df.transaction_date.dt.dayofyear
    df['week_of_year'] = df.transaction_date.dt.weekofyear
    df['day_of_week'] = df.transaction_date.dt.dayofweek
    df['year'] = df.transaction_date.dt.year
    df["is_wknd"] = df.transaction_date.dt.weekday // 4
    df['is_month_start'] = df.transaction_date.dt.is_month_start.astype(int)
    df['is_month_end'] = df.transaction_date.dt.is_month_end.astype(int)
    return df

df = create_date_features(df)
df.head()

#more visulation 

sns.boxplot(data = df, x = df["month"], y = df["Total_Transaction"])
plt.show(block=True)

sns.boxplot(data = df, x = df["year"], y = df["Total_Transaction"])
plt.show(block=True)

sns.boxplot(data = df, x = df["is_wknd"], y = df["Total_Transaction"])
plt.show(block=True)

sns.boxplot(data = df, x = df["day_of_month"], y = df["Total_Transaction"])
plt.show(block=True)

#sorted by merchant id and date
df.sort_values(by=['merchant_id', 'transaction_date'], axis=0).head()

#Sıralanmıs veri ile farklı gecikmelere aİt faturelar oluşturuyoruz.
pd.DataFrame({"Total_Transaction": df["Total_Paid"].values[0:10],
              "lag1": df["Total_Transaction"].shift(1).values[0:10],
              "lag2": df["Total_Transaction"].shift(2).values[0:10],
              "lag3": df["Total_Transaction"].shift(3).values[0:10],
              "lag4": df["Total_Transaction"].shift(4).values[0:10]})


#adding noise to data to avoid overfitting
def random_noise(dataframe):
    return np.random.normal(scale=1.6, size=(len(dataframe),))


def lag_features(dataframe, lags):
    for lag in lags:
        dataframe['sales_lag_' + str(lag)] = dataframe.groupby(["merchant_id"])['Total_Transaction'].transform(
            lambda x: x.shift(lag)) + random_noise(dataframe)
    return dataframe

df = lag_features(df, [91, 120, 152, 182, 242, 402, 542, 722])

check_df(df)

pd.DataFrame({'Total_Transaction': df['Total_Transaction'].values[0:10],
              "roll2": df['Total_Transaction'].shift(1).rolling(window=2).mean().values[0:10],
              "roll3": df['Total_Transaction'].shift(1).rolling(window=3).mean().values[0:10],
              "roll5": df['Total_Transaction'].shift(1).rolling(window=5).mean().values[0:10]})

def roll_mean_features(dataframe, windows):
    for window in windows:
        dataframe['sales_roll_mean_' + str(window)] = dataframe.groupby(["merchant_id"])['Total_Transaction']. \
                                                          transform(
            lambda x: x.shift(1).rolling(window=window, min_periods=10, win_type="triang").mean()) + random_noise(
            dataframe)
    return dataframe

df = roll_mean_features(df, [91, 120, 152, 182, 242, 402, 542, 722])

# Exponentially Weighted Mean Features

pd.DataFrame({"'Total_Transaction'": df['Total_Paid'].values[0:10],
              "roll2": df['Total_Transaction'].shift(1).rolling(window=2).mean().values[0:10],
              "ewm099": df['Total_Transaction'].shift(1).ewm(alpha=0.99).mean().values[0:10],
              "ewm095": df['Total_Transaction'].shift(1).ewm(alpha=0.95).mean().values[0:10],
              "ewm07": df['Total_Transaction'].shift(1).ewm(alpha=0.7).mean().values[0:10],
              "ewm02": df['Total_Transaction'].shift(1).ewm(alpha=0.1).mean().values[0:10]})

def ewm_features(dataframe, alphas, lags):
    for alpha in alphas:
        for lag in lags:
            dataframe['sales_ewm_alpha_' + str(alpha).replace(".", "") + "_lag_" + str(lag)] = \
                dataframe.groupby("merchant_id")['Total_Transaction'].transform(lambda x: x.shift(lag).ewm(alpha=alpha).mean())
    return dataframe

alphas = [0.95, 0.9, 0.8, 0.7, 0.5]
lags = [91, 120, 152, 182, 242, 402, 542, 722]

df = ewm_features(df, alphas, lags)

check_df(df)

def grab_col_names(dataframe, cat_th=10, car_th=20):
    # cat_cols, cat_but_car
    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == "O"]
    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and
                   dataframe[col].dtypes != "O"]
    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and
                   dataframe[col].dtypes == "O"]
    cat_cols = cat_cols + num_but_cat
    cat_cols = [col for col in cat_cols if col not in cat_but_car]

    # num_cols
    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != "O"]
    num_cols = [col for col in num_cols if col not in num_but_cat]

    print(f"Observations: {dataframe.shape[0]}")
    print(f"Variables: {dataframe.shape[1]}")
    print(f'cat_cols: {len(cat_cols)}')
    print(f'num_cols: {len(num_cols)}')
    print(f'cat_but_car: {len(cat_but_car)}')
    print(f'num_but_cat: {len(num_but_cat)}')
    return cat_cols, num_cols, cat_but_car
cat_cols, num_cols, cat_but_car = grab_col_names(df)

num_cols
cat_cols

########################
# ENCODING (ONE HOT ENCODING)
########################
df.head()
df = pd.get_dummies(df, columns=['day_of_week','year'])
df.shape
#(7667, 523)

########################
# CUSTOM COST FUNCTION
########################

# MAE, MSE, RMSE, SSE, SMAPE

# MAE: mean absolute error
# MAPE: mean absolute percentage error
# SMAPE: Symmetric mean absolute percentage error (adjusted MAPE)

def smape(preds, target):
    n = len(preds)
    masked_arr = ~((preds == 0) & (target == 0))
    preds, target = preds[masked_arr], target[masked_arr]
    num = np.abs(preds - target)
    denom = np.abs(preds) + np.abs(target)
    smape_val = (200 * np.sum(num / denom)) / n
    return smape_val

def lgbm_smape(preds, train_data):
    labels = train_data.get_label()
    smape_val = smape(np.expm1(preds), np.expm1(labels))
    return 'SMAPE', smape_val, False

########################
# Converting sales to log(1+sales)
########################

df['Total_Transaction'] = np.log1p(df["Total_Transaction"].values)

########################
# Time-Based Validation Sets
########################

# To last of 2019 is train set.
df.shape
train = df.loc[(df["transaction_date"] < "2020-01-01"), :]

train.head()

#first 3 monts of 2020 is validation set. 
val = df.loc[(df["transaction_date"] >= "2020-01-01") & (df["transaction_date"] < "2020-04-01"), :]

val.head()
train.info()

We lost the information of 9 monts of 2020 because of apply seasonalty to data.

cols = [col for col in train.columns if col not in ['transaction_date',"Total_Transaction","Total_Paid", "year"]]

Y_train = train['Total_Transaction']
X_train = train[cols]

Y_val = val['Total_Transaction']
X_val = val[cols]

Y_train.shape, X_train.shape, Y_val.shape, X_val.shape
# Out[279]: ((4251,), (4251, 95), (847,), (847, 95))


########################
# Time Series Model with LightGBM
########################

# !pip install lightgbm
# conda install lightgbm


# LightGBM parameters
lgb_params = {'num_leaves': 10,
              'learning_rate': 0.02,
              'feature_fraction': 0.8,
              'max_depth': 5,
              'verbose': 0,
              'num_boost_round': 10000,
              'early_stopping_rounds': 200,
              'nthread': -1}

# metric mae: l1, absolute loss, mean_absolute_error, regression_l1
# mse: l2, square loss, mean_squared_error, mse, regression_l2, regression
# rmse, root square loss, root_mean_squared_error, l2_root
# mape, MAPE loss, mean_absolute_percentage_error

lgbtrain = lgb.Dataset(data=X_train, label=Y_train, feature_name=cols)

lgbval = lgb.Dataset(data=X_val, label=Y_val, reference=lgbtrain, feature_name=cols)

model = lgb.train(lgb_params, lgbtrain,
                  valid_sets=[lgbtrain, lgbval],
                  num_boost_round=lgb_params['num_boost_round'],
                  early_stopping_rounds=lgb_params['early_stopping_rounds'],
                  feval=lgbm_smape,
                  verbose_eval=100)


y_pred_val = model.predict(X_val, num_iteration=model.best_iteration)

smape(np.expm1(y_pred_val), np.expm1(Y_val))
# 24.822

#if we set the time of data as 28/8 months as train and validation; we are goning to get SMAPE: 32.93507139156651 (residual scor)
#so that is why we set the data with seasonality.


########################
# Feature Importance
########################

def plot_lgb_importances(model, plot=False, num=10):
    gain = model.feature_importance('gain')
    feat_imp = pd.DataFrame({'feature': model.feature_name(),
                             'split': model.feature_importance('split'),
                             'gain': 100 * gain / gain.sum()}).sort_values('gain', ascending=False)
    if plot:
        plt.figure(figsize=(10, 10))
        sns.set(font_scale=1)
        sns.barplot(x="gain", y="feature", data=feat_imp[0:25])
        plt.title('feature')
        plt.tight_layout()
        plt.show(block=True)
    else:
        print(feat_imp.head(num))
    return feat_imp

plot_lgb_importances(model, num=200)

plot_lgb_importances(model, num=30, plot=True)

########################
# Final Model
########################

cols = [col for col in df.columns if col not in ["Total_Transaction","transaction_date"]]
Y_train = df['Total_Transaction']
X_train = df[cols]

Y_train.shape,X_train.shape

lgb_params = {'num_leaves': 20,
              'learning_rate': 0.02,
              'feature_fraction': 0.9,
              'max_depth': 15,
              'verbose': 0,
              'num_boost_round': model.best_iteration,
              'nthread': -1}

lgbtrain_all = lgb.Dataset(data=X_train, label=Y_train, feature_name=cols)

final_model = lgb.train(lgb_params, lgbtrain_all, num_boost_round=model.best_iteration)

test_preds = final_model.predict(X_val, predict_disable_shape_check=True, num_iteration=model.best_iteration)


########################
# Submission File
########################
df.head()
val.head()

submission_df = val.loc[:, ["merchant_id", "Total_Paid"]]

submission_df['Total_Paid'] = np.expm1(test_preds)

submission_df['merchant_id'] = submission_df['merchant_id'].astype(int)

submission_df.to_csv("submission_demand.csv", index=False)
